{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b999983",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Understanding\n",
    "For this project, we are utilizizng a dataset from Movielens dataset from Grouplens research lab at University of Minnesota. The dataset (ml-latest-small) describes 5-star rating and free-text tagging activity. It contains 4 files: movies.csv, ratings.csv, links.csv, tags.csv.\n",
    "\n",
    "The various file have different columns within them:\n",
    "\n",
    "Moives.csv has the following columns:\n",
    "\n",
    "movieid : is an identifier for movies.\n",
    "tile : Represents the title of the movies.\n",
    "genre : Represents the type of movie.\n",
    "Ratings.csv has the following columns:\n",
    "\n",
    "userid : is an identifier for users.\n",
    "movieid : is an identifier for movies.\n",
    "rating : are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "timestamp : represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "Links.csv has the following columns:\n",
    "\n",
    "movieid : is an identifier for movies.\n",
    "imdbid : is an identifier for movies used by http://www.imdb.com.\n",
    "tmdbid : is an identifier for movies used by https://www.themoviedb.org.\n",
    "Tags.csv has the following columns:\n",
    "\n",
    "userid : is an identifier for users.\n",
    "movieid : is an identifier for movies.\n",
    "tag : are user-generated metadata about movies. Each tag is typically a single word or short phrase\n",
    "timestamp : represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "To start the analysis first we need to import the necessary libraries that will be essential for functions like data manipulation, visualization, and statistical analysis.\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "The next step involves opening the datasets and examining the first five rows of the dataset to gain initial understanding of its structure.\n",
    "\n",
    "# Reading the movies dataset \n",
    "df_movies = pd.read_csv('ml-latest-small/ml-latest-small/movies.csv')\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\n",
    "# Reading the ratings dataset \n",
    "df_ratings = pd.read_csv('ml-latest-small/ml-latest-small/ratings.csv')\n",
    "df_ratings.head()\n",
    "userId\tmovieId\trating\ttimestamp\n",
    "0\t1\t1\t4.0\t964982703\n",
    "1\t1\t3\t4.0\t964981247\n",
    "2\t1\t6\t4.0\t964982224\n",
    "3\t1\t47\t5.0\t964983815\n",
    "4\t1\t50\t5.0\t964982931\n",
    "# Reading the links dataset \n",
    "df_links = pd.read_csv('ml-latest-small/ml-latest-small/links.csv')\n",
    "df_links.head()\n",
    "movieId\timdbId\ttmdbId\n",
    "0\t1\t114709\t862.0\n",
    "1\t2\t113497\t8844.0\n",
    "2\t3\t113228\t15602.0\n",
    "3\t4\t114885\t31357.0\n",
    "4\t5\t113041\t11862.0\n",
    "# Reading the tags dataset \n",
    "df_tags = pd.read_csv('ml-latest-small/ml-latest-small/tags.csv')\n",
    "df_tags.head()\n",
    "userId\tmovieId\ttag\ttimestamp\n",
    "0\t2\t60756\tfunny\t1445714994\n",
    "1\t2\t60756\tHighly quotable\t1445714996\n",
    "2\t2\t60756\twill ferrell\t1445714992\n",
    "3\t2\t89774\tBoxing story\t1445715207\n",
    "4\t2\t89774\tMMA\t1445715200\n",
    "To delve deeper into the data, we've created several functions:\n",
    "\n",
    "data_shape(data): This function tells us how big the dataset is. It provides the number of rows and columns in the dataset.\n",
    "\n",
    "data_info(data): This function gives us a snapshot of the data. It shows us the names of the columns, what kind of data they hold, and how many non-empty values there are in each column.\n",
    "\n",
    "data_missing(data): This function checks if there are any gaps in our data, meaning missing values in any columns. If everything is complete, it lets us know that no data is missing.\n",
    "\n",
    "identify_duplicates(data): This function helps us spot any identical rows in the dataset. It tells us how many duplicate rows there are and what percentage of the dataset they make up. If there are no duplicates, it tells us that.\n",
    "\n",
    "unique_column_duplicates(data, column): This function specifically looks for duplicates in a particular column. It counts them and calculates what percentage of the data they represent. If the column has no duplicates, it mentions that.\n",
    "\n",
    "data_describe(data): This function summarizes the numerical aspects of our data. It gives us statistics like the average, standard deviation, minimum, quartiles, and maximum values for numerical columns.\n",
    "\n",
    "These functions are like tools to help us better understand our data's structure, find missing information, identify repeated entries, and get an overview of the numbers in our dataset.\n",
    "\n",
    "We can use these functions together under a single function called \"explore_data()\" to perform a thorough exploration of our dataset.\n",
    "\n",
    "# Shape of the data \n",
    "def df_shape(dataframe_name, df):\n",
    "    num_rows, num_columns = df.shape\n",
    "    print(f\"{dataframe_name} has {num_rows} rows and {num_columns} columns.\")\n",
    "​\n",
    "​\n",
    "# function to find the info for the dataset \n",
    "def df_info(dataframe_name, df):\n",
    "    print(\"\\n\")\n",
    "    print(f\"Info for {dataframe_name}:\\n\")\n",
    "    print(df.info())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Function to find the missing values in the dataset \n",
    "def df_missing(dataframe_name, df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing Values in {dataframe_name}:\")\n",
    "    \n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"No missing values.\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        \n",
    "​\n",
    "# df_missing(\"Movies DataFrame\", df_movies)\n",
    "# df_missing(\"Ratings DataFrame\", df_ratings)\n",
    "# Function to find duplicated values of the rows in the dataset \n",
    "def df_duplicated(dataframe_name, df):\n",
    "    duplicated_rows = df[df.duplicated()]\n",
    "    print(f\"Duplicated rows in {dataframe_name}:\")\n",
    "    \n",
    "    if duplicated_rows.shape[0] == 0:\n",
    "        print(\"No duplicate rows.\\n\")\n",
    "        \n",
    "    else: \n",
    "        print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
    "        print(\"\\nSample of duplicate rows:\")\n",
    "        print(duplicate_rows.head())\n",
    "        \n",
    "# df_duplicates(\"Movies DataFrame\", df_movies)\n",
    "# df_duplicates(\"Ratings DataFrame\", df_ratings)\n",
    "​\n",
    "# Function to find duplicated values of the columns in the dataset \n",
    "def column_duplicates(dataframe_name, df):\n",
    "    duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "    print(f\"Duplicated columns in {dataframe_name}:\")\n",
    "    \n",
    "    if len(duplicate_columns) == 0:\n",
    "        print(\"No duplicate columns found.\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Duplicate columns: {', '.join(duplicate_columns)}\")\n",
    "          \n",
    "# column_duplicates(\"Movies DataFrame\", df_movies)\n",
    "# column_duplicates(\"Ratings DataFrame\", df_ratings)\n",
    "​\n",
    "# Function to print out descriptive stats \n",
    "def df_describe(dataframe_name, df):\n",
    "    print(f\"Descriptive Statistics for {dataframe_name}:\")\n",
    "    print(df.describe(),\"\\n\")\n",
    "    \n",
    "# df_describe(\"Movies DataFrame\", df_movies)\n",
    "# df_describe(\"Ratings DataFrame\", df_ratings) \n",
    "# Function to explore the data \n",
    "def explore_df(dataframe_name, df):\n",
    "    df_shape(\"Movies DataFrame\", df_movies), df_shape(\"Ratings DataFrame\", df_ratings)\n",
    "​\n",
    "    df_info(\"Movies DataFrame\", df_movies), df_info(\"Ratings DataFrame\", df_ratings)\n",
    "    \n",
    "    df_missing(\"Movies DataFrame\", df_movies), df_missing(\"Ratings DataFrame\", df_ratings)\n",
    "    \n",
    "    df_duplicated(\"Movies DataFrame\", df_movies), df_duplicated(\"Ratings DataFrame\", df_ratings)\n",
    "    \n",
    "    column_duplicates(\"Movies DataFrame\", df_movies), column_duplicates(\"Ratings DataFrame\", df_ratings)\n",
    "    \n",
    "    df_describe(\"Movies DataFrame\", df_movies), df_describe(\"Ratings DataFrame\", df_ratings)\n",
    "    \n",
    "# call the function explore_data\n",
    "# explore_data(dataframe_name, df)\n",
    "​\n",
    "explore_df(\"Movies DataFrame\", df_movies)\n",
    "# explore_df(\"Ratings DataFrame\", df_ratings)\n",
    "​\n",
    "​\n",
    "Movies DataFrame has 9742 rows and 3 columns.\n",
    "Ratings DataFrame has 100836 rows and 4 columns.\n",
    "\n",
    "\n",
    "Info for Movies DataFrame:\n",
    "\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 9742 entries, 0 to 9741\n",
    "Data columns (total 3 columns):\n",
    " #   Column   Non-Null Count  Dtype \n",
    "---  ------   --------------  ----- \n",
    " 0   movieId  9742 non-null   int64 \n",
    " 1   title    9742 non-null   object\n",
    " 2   genres   9742 non-null   object\n",
    "dtypes: int64(1), object(2)\n",
    "memory usage: 228.5+ KB\n",
    "None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Info for Ratings DataFrame:\n",
    "\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 100836 entries, 0 to 100835\n",
    "Data columns (total 4 columns):\n",
    " #   Column     Non-Null Count   Dtype  \n",
    "---  ------     --------------   -----  \n",
    " 0   userId     100836 non-null  int64  \n",
    " 1   movieId    100836 non-null  int64  \n",
    " 2   rating     100836 non-null  float64\n",
    " 3   timestamp  100836 non-null  int64  \n",
    "dtypes: float64(1), int64(3)\n",
    "memory usage: 3.1 MB\n",
    "None\n",
    "\n",
    "\n",
    "Missing Values in Movies DataFrame:\n",
    "No missing values.\n",
    "\n",
    "Missing Values in Ratings DataFrame:\n",
    "No missing values.\n",
    "\n",
    "Duplicated rows in Movies DataFrame:\n",
    "No duplicate rows.\n",
    "\n",
    "Duplicated rows in Ratings DataFrame:\n",
    "No duplicate rows.\n",
    "\n",
    "Duplicated columns in Movies DataFrame:\n",
    "No duplicate columns found.\n",
    "\n",
    "Duplicated columns in Ratings DataFrame:\n",
    "No duplicate columns found.\n",
    "\n",
    "Descriptive Statistics for Movies DataFrame:\n",
    "             movieId\n",
    "count    9742.000000\n",
    "mean    42200.353623\n",
    "std     52160.494854\n",
    "min         1.000000\n",
    "25%      3248.250000\n",
    "50%      7300.000000\n",
    "75%     76232.000000\n",
    "max    193609.000000 \n",
    "\n",
    "Descriptive Statistics for Ratings DataFrame:\n",
    "              userId        movieId         rating     timestamp\n",
    "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
    "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
    "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
    "min         1.000000       1.000000       0.500000  8.281246e+08\n",
    "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
    "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
    "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
    "max       610.000000  193609.000000       5.000000  1.537799e+09 \n",
    "\n",
    "Data preparation\n",
    "The movie dataset contains 3 columns with 1 numeric variable and 2 object variable. The ratings dataset contains 4 columns with 3 numeric variables and 1 object columns.\n",
    "\n",
    "Here we will convert the timestamp to human redable format:\n",
    "\n",
    "​\n",
    "# Function to convert ratings timestamps to human-readable format\n",
    "def convert_timestamp(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "​\n",
    "# Apply the function to the timestamp column\n",
    "df_ratings['timestamp_readable'] = df_ratings['timestamp'].apply(convert_timestamp)\n",
    "​\n",
    "# Print the DataFrame with the readable timestamp\n",
    "print(df_ratings[['userId', 'movieId', 'rating', 'timestamp', 'timestamp_readable']].head())\n",
    "​\n",
    "   userId  movieId  rating  timestamp   timestamp_readable\n",
    "0       1        1     4.0  964982703  2000-07-30 18:45:03\n",
    "1       1        3     4.0  964981247  2000-07-30 18:20:47\n",
    "2       1        6     4.0  964982224  2000-07-30 18:37:04\n",
    "3       1       47     5.0  964983815  2000-07-30 19:03:35\n",
    "4       1       50     5.0  964982931  2000-07-30 18:48:51\n",
    "To check for outliers here is not important since the context of the datasets provided. There are no numerical columns with continuous values that are typically checked for outliers. Instead, the primary numerical columns are \"movieId\", \"userId\", \"rating\", and \"timestamp\", but they are identifiers or timestamos and are not sutable for outliers detection.\n",
    "\n",
    "Feature Engineering\n",
    "Feature engineering is the process of creating new features or modifying existing features in a dataset to improve the perfomance of machine learnig models.\n",
    "\n",
    "feature engineering in movies.csv file\n",
    "# Feature engineering for df_movies DataFrame\n",
    "# Example: Extracting the release year\n",
    "df_movies['release_year'] = df_movies['title'].str.extract(r'\\((\\d{4})\\)$').astype(float)\n",
    "​\n",
    "# # Example: Encoding genres\n",
    "# genres = df_movies['genres'].str.get_dummies('|')\n",
    "# df_movies = pd.concat([df_movies, genres], axis=1)\n",
    "​\n",
    "# # Feature engineering for df_ratings DataFrame\n",
    "# # Example: Extracting year and month from the timestamp\n",
    "# df_ratings['year'] = df_ratings['timestamp'].apply(lambda x: datetime.utcfromtimestamp(x).year)\n",
    "# df_ratings['month'] = df_ratings['timestamp'].apply(lambda x: datetime.utcfromtimestamp(x).month)\n",
    "​\n",
    "# # Example: Calculating average rating per movie\n",
    "# average_rating_per_movie = df_ratings.groupby('movieId')['rating'].mean()\n",
    "# df_movies['average_rating'] = df_movies['movieId'].map(average_rating_per_movie)\n",
    "​\n",
    "# # Example: Calculating rating count per movie\n",
    "# rating_count_per_movie = df_ratings.groupby('movieId')['rating'].count()\n",
    "# df_movies['rating_count'] = df_movies['movieId'].map(rating_count_per_movie)\n",
    "​\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\trelease_year\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\t1995.0\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\t1995.0\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\t1995.0\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\t1995.0\n",
    "df_movies['genres'].head()\n",
    "0    Adventure|Animation|Children|Comedy|Fantasy\n",
    "1                     Adventure|Children|Fantasy\n",
    "2                                 Comedy|Romance\n",
    "3                           Comedy|Drama|Romance\n",
    "4                                         Comedy\n",
    "Name: genres, dtype: object\n",
    "# # splitting the genres into different columns \n",
    "# genres = df_movies['genres'].str.get_dummies('|')\n",
    "​\n",
    "# # Concatenate the one-hot encoded genres with the original DataFrame\n",
    "# df_movies = pd.concat([df_movies, genres], axis=1)\n",
    "​\n",
    "# # Drop the original 'genres' column\n",
    "# # df_movies = df_movies.drop('genres', axis=1)\n",
    "# df_movies.head()\n",
    "# Extracting the release year and creating a new column 'release_year'\n",
    "df_movies['release_year'] = df_movies['title'].str.extract(r'\\((\\d{4})\\)$').astype(float)\n",
    "​\n",
    "# Removing the year from the 'title' column\n",
    "df_movies['title'] = df_movies['title'].str.replace(r'\\(\\d{4}\\)', '').str.strip()\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\trelease_year\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\t1995.0\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\t1995.0\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\t1995.0\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\t1995.0\n",
    "# Remember to interpret the code !!\n",
    "​\n",
    "# Example: Calculating average rating per movie\n",
    "average_rating_per_movie = df_ratings.groupby('movieId')['rating'].mean()\n",
    "df_movies['average_rating'] = df_movies['movieId'].map(average_rating_per_movie)\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\trelease_year\taverage_rating\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.920930\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\t1995.0\t3.431818\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\t1995.0\t3.259615\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\t1995.0\t2.357143\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\t1995.0\t3.071429\n",
    "The code above calculates the average rating per movie and adds this information as a new feature ('average_rating') to the 'df_movies' DataFrame.\n",
    "\n",
    "​\n",
    "# Remember to interpret \n",
    "​\n",
    "# # Example: Calculating rating count per movie\n",
    "rating_count_per_movie = df_ratings.groupby('movieId')['rating'].count()\n",
    "df_movies['rating_count'] = df_movies['movieId'].map(rating_count_per_movie)\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\trelease_year\taverage_rating\trating_count\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.920930\t215.0\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\t1995.0\t3.431818\t110.0\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\t1995.0\t3.259615\t52.0\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\t1995.0\t2.357143\t7.0\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\t1995.0\t3.071429\t49.0\n",
    "The code provided above calcuates the rating count per movie and adds this information as a new feature ('rating_count') to the 'df_movies' DataFrame.\n",
    "\n",
    "Multivariante Analysis\n",
    "Multivariante analysis involves examining the relationship between multiplevariables in dataset. IN the context of my 'df_movies' and 'df_ratings' DataFrames, you can perform multivariate analysis to explore how different movies attributes(e.g., genre, release_year) a nd the user behavior(ratings) interact with each other.\n",
    "\n",
    "Multivariate Analysis on 'df_movies' (Movie Attributes)\n",
    "Correlation analysis\n",
    "Used to calculate and visualize the correlation between numerical variables in the 'df_movies' DataFrame. In the context of this dataset I can use a correlation matrix and a heatmap to see how attributes like 'release_year' and 'average_rating' are related.\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_movies[['release_year', 'average_rating']].corr()\n",
    "​\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap for Movie Attributes')\n",
    "plt.show()\n",
    "​\n",
    "\n",
    "The correlation matrix you've calculated and visualized provides insights into the linear relationships (correlations) between the 'release_year' and 'average_rating' attributes in the df_movies DataFrame. Here's how to interpret the correlation matrix:\n",
    "\n",
    "Correlation Values: Each cell in the matrix represents the correlation coefficient between two variables. The correlation coefficient ranges from -1 to 1, where:\n",
    "\n",
    "A value of 1 indicates a perfect positive linear relationship. When one variable increases, the other also increases by a constant factor. A value of -1 indicates a perfect negative linear relationship. When one variable increases, the other decreases by a constant factor. A value close to 0 indicates a weak or no linear relationship. There is little to no linear association between the variables. Color Mapping: In the heatmap, the colors represent the correlation values. A color scale is typically used, where warmer colors (e.g., red or orange) indicate positive correlations, and cooler colors (e.g., blue) indicate negative correlations.\n",
    "\n",
    "Now, let's interpret the specific elements of your correlation matrix:\n",
    "\n",
    "Correlation between 'release_year' and 'average_rating': The correlation coefficient measures how 'release_year' and 'average_rating' are related.\n",
    "\n",
    "If the correlation coefficient is close to 1: It suggests a strong positive linear relationship. In your case, it would mean that as 'release_year' increases, 'average_rating' tends to increase as well, indicating that newer movies tend to have higher average ratings.\n",
    "\n",
    "If the correlation coefficient is close to -1: It suggests a strong negative linear relationship. In your case, it would mean that as 'release_year' increases, 'average_rating' tends to decrease, indicating that older movies tend to have higher average ratings.\n",
    "\n",
    "If the correlation coefficient is close to 0: It suggests little to no linear relationship between the two variables. In your case, it would mean that there is no strong linear relationship between 'release_year' and 'average_rating,' indicating that the release year may not be a significant predictor of average ratings.\n",
    "\n",
    "In your code, you've used the 'coolwarm' colormap, where negative correlations appear in blue, and positive correlations appear in red. The 'annot=True' parameter adds the actual correlation values to the heatmap cells for reference.\n",
    "\n",
    "To summarize, the correlation heatmap helps you visually identify whether there is a linear relationship between 'release_year' and 'average_rating' and the strength and direction of that relationship. In your case, it appears that there may be a slight negative correlation between the two variables, suggesting that older movies tend to have slightly higher average ratings. However, the correlation is not very strong, so other factors may also influence movie ratings.\n",
    "\n",
    "Genre Analysis:\n",
    "Explore the relationships between movie genres and user ratings. You can calculate the average rating for each genre and visualize which genres tend to receive higher or lower ratings.\n",
    "\n",
    "# genres  .value_counts()\n",
    "# Calculate average ratings per genre\n",
    "genre_ratings = df_movies.groupby('genres')['average_rating'].mean()\n",
    "​\n",
    "# Sort genres by average rating in descending order\n",
    "sorted_genre_ratings = genre_ratings.sort_values(ascending=False)\n",
    "​\n",
    "# Select the top 5 genres\n",
    "top_5_genres = sorted_genre_ratings.head(5)\n",
    "​\n",
    "# Select the bottom 5 genres\n",
    "bottom_5_genres = sorted_genre_ratings.tail(5)\n",
    "​\n",
    "# Concatenate the top 5 and bottom 5 genres\n",
    "top_and_bottom_genres = pd.concat([top_5_genres, bottom_5_genres])\n",
    "​\n",
    "# Visualize the top and bottom genres by average rating\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_and_bottom_genres.plot(kind='bar')\n",
    "plt.title('Top and Bottom Genres by Average Rating')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xticks(rotation=90)  # Adjust the rotation of x-axis labels for better readability\n",
    "plt.show()\n",
    "​\n",
    "\n",
    "Time Series Analysis:\n",
    "If you have a time-based attribute like 'release_year,' you can perform time series analysis to understand how movie release years have evolved over time\n",
    "\n",
    "# Plot a line chart to show the count of movies released each year\n",
    "df_movies['release_year'].value_counts().sort_index().plot(kind='line')\n",
    "plt.title('Movie Release Years Over Time')\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "​\n",
    "\n",
    "The line chart you've created shows the count of movies released each year over a period of time. It's a form of time series analysis that helps you visualize how movie release years have evolved over time. Here's how to interpret the chart:\n",
    "\n",
    "X-Axis (Release Year): The horizontal axis represents the years in which movies were released. Each point on the X-axis corresponds to a specific year.\n",
    "\n",
    "Y-Axis (Count): The vertical axis represents the count of movies released in each year. The Y-axis values indicate how many movies were released in a particular year.\n",
    "\n",
    "Line Plot: The line on the chart connects the data points for each year, showing the trend in movie releases over time.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Upward and Downward Trends: You can observe whether there are upward or downward trends in movie releases. An upward trend would indicate an increase in the number of movies released each year, while a downward trend would suggest a decrease.\n",
    "\n",
    "Peaks and Valleys: Peaks in the line indicate years with a high number of movie releases, while valleys represent years with fewer releases.\n",
    "\n",
    "Cyclic Patterns: Look for any cyclic patterns or periodic fluctuations in the chart. For example, there might be a pattern where movie releases increase for a few years and then decrease before increasing again.\n",
    "\n",
    "Outliers: Identify any outliers or years that deviate significantly from the overall trend. These outliers could represent exceptional years with a significantly higher or lower number of releases.\n",
    "\n",
    "Overall Movie Release Trends: Based on the chart, you can draw conclusions about the overall trends in movie production over time. For example, you might observe that movie production has been steadily increasing over the years or that there are certain decades with particularly high movie production.\n",
    "\n",
    "This type of time series analysis can provide valuable insights into how movie release patterns have changed over time and can be useful for historical or industry trend analysis in the film industry.\n",
    "\n",
    "Multivariate Analysis on df_ratings (User Behavior):\n",
    "User Behavior by Genre : Explore how users' ratings vary by movie genre. You can calculate average ratings for each genre and see if there are genre preferences among users.\n",
    "\n",
    "Here is where I merge the two datasets df_movies and df_ratings into df_merged.\n",
    "\n",
    "Time Series Analysis:\n",
    "Analyze how user ratings have evolved over time. You can plot the average ratings or the count of ratings per year to identify trends.\n",
    "\n",
    "# Convert 'timestamp' to a datetime object\n",
    "df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'], unit='s')\n",
    "​\n",
    "# Extract the year from the timestamp\n",
    "df_ratings['year'] = df_ratings['timestamp'].dt.year\n",
    "​\n",
    "# Calculate average ratings per year\n",
    "average_ratings_by_year = df_ratings.groupby('year')['rating'].mean()\n",
    "​\n",
    "# Visualize average ratings over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "average_ratings_by_year.plot(kind='line')\n",
    "plt.title('Average Ratings Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n",
    "​\n",
    "\n",
    "hi\n",
    "\n",
    "# Convert 'timestamp' to a datetime object\n",
    "df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'], unit='s')\n",
    "​\n",
    "# Extract the year from the timestamp\n",
    "df_ratings['year'] = df_ratings['timestamp'].dt.year\n",
    "​\n",
    "# Calculate the count of ratings per year\n",
    "ratings_count_by_year = df_ratings.groupby('year')['rating'].count()\n",
    "​\n",
    "# Visualize ratings count over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "ratings_count_by_year.plot(kind='line')\n",
    "plt.title('Ratings Count Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.show()\n",
    "​\n",
    "\n",
    "Feature selection part\n",
    "df_movies.head()\n",
    "movieId\ttitle\tgenres\trelease_year\taverage_rating\trating_count\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.920930\t215.0\n",
    "1\t2\tJumanji (1995)\tAdventure|Children|Fantasy\t1995.0\t3.431818\t110.0\n",
    "2\t3\tGrumpier Old Men (1995)\tComedy|Romance\t1995.0\t3.259615\t52.0\n",
    "3\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\t1995.0\t2.357143\t7.0\n",
    "4\t5\tFather of the Bride Part II (1995)\tComedy\t1995.0\t3.071429\t49.0\n",
    "df_ratings.head()\n",
    "userId\tmovieId\trating\ttimestamp\ttimestamp_readable\tyear\n",
    "0\t1\t1\t4.0\t2000-07-30 18:45:03\t2000-07-30 18:45:03\t2000\n",
    "1\t1\t3\t4.0\t2000-07-30 18:20:47\t2000-07-30 18:20:47\t2000\n",
    "2\t1\t6\t4.0\t2000-07-30 18:37:04\t2000-07-30 18:37:04\t2000\n",
    "3\t1\t47\t5.0\t2000-07-30 19:03:35\t2000-07-30 19:03:35\t2000\n",
    "4\t1\t50\t5.0\t2000-07-30 18:48:51\t2000-07-30 18:48:51\t2000\n",
    "poejgw\n",
    "\n",
    "df_merged = df_movies.merge(df_ratings, how=\"left\", on='movieId')\n",
    "df_merged\n",
    "movieId\ttitle\tgenres\trelease_year\taverage_rating\trating_count\tuserId\trating\ttimestamp\ttimestamp_readable\tyear\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t1.0\t4.0\t2000-07-30 18:45:03\t2000-07-30 18:45:03\t2000.0\n",
    "1\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t5.0\t4.0\t1996-11-08 06:36:02\t1996-11-08 06:36:02\t1996.0\n",
    "2\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t7.0\t4.5\t2005-01-25 06:52:26\t2005-01-25 06:52:26\t2005.0\n",
    "3\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t15.0\t2.5\t2017-11-13 12:59:30\t2017-11-13 12:59:30\t2017.0\n",
    "4\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t17.0\t4.5\t2011-05-18 05:28:03\t2011-05-18 05:28:03\t2011.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "100849\t193581\tBlack Butler: Book of the Atlantic (2017)\tAction|Animation|Comedy|Fantasy\t2017.0\t4.00000\t1.0\t184.0\t4.0\t2018-09-16 14:44:42\t2018-09-16 14:44:42\t2018.0\n",
    "100850\t193583\tNo Game No Life: Zero (2017)\tAnimation|Comedy|Fantasy\t2017.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 14:52:25\t2018-09-16 14:52:25\t2018.0\n",
    "100851\t193585\tFlint (2017)\tDrama\t2017.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 14:56:45\t2018-09-16 14:56:45\t2018.0\n",
    "100852\t193587\tBungo Stray Dogs: Dead Apple (2018)\tAction|Animation\t2018.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 15:00:21\t2018-09-16 15:00:21\t2018.0\n",
    "100853\t193609\tAndrew Dice Clay: Dice Rules (1991)\tComedy\t1991.0\t4.00000\t1.0\t331.0\t4.0\t2018-09-17 04:13:26\t2018-09-17 04:13:26\t2018.0\n",
    "100854 rows × 11 columns\n",
    "\n",
    "df_merged.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 100854 entries, 0 to 100853\n",
    "Data columns (total 11 columns):\n",
    " #   Column              Non-Null Count   Dtype         \n",
    "---  ------              --------------   -----         \n",
    " 0   movieId             100854 non-null  int64         \n",
    " 1   title               100854 non-null  object        \n",
    " 2   genres              100854 non-null  object        \n",
    " 3   release_year        100836 non-null  float64       \n",
    " 4   average_rating      100836 non-null  float64       \n",
    " 5   rating_count        100836 non-null  float64       \n",
    " 6   userId              100836 non-null  float64       \n",
    " 7   rating              100836 non-null  float64       \n",
    " 8   timestamp           100836 non-null  datetime64[ns]\n",
    " 9   timestamp_readable  100836 non-null  object        \n",
    " 10  year                100836 non-null  float64       \n",
    "dtypes: datetime64[ns](1), float64(6), int64(1), object(3)\n",
    "memory usage: 8.5+ MB\n",
    "df_merged.dropna(inplace=True)\n",
    "# Check for missing values in the df_merged DataFrame\n",
    "missing_values = df_merged.isna().sum()\n",
    "​\n",
    "# Display columns with missing values (if any)\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "if not columns_with_missing_values.empty:\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    print(columns_with_missing_values)\n",
    "else:\n",
    "    print(\"No Missing Values Found in the Dataset\")\n",
    "No Missing Values Found in the Dataset\n",
    "df_ratings.head()\n",
    "userId\tmovieId\trating\ttimestamp\ttimestamp_readable\tyear\n",
    "0\t1\t1\t4.0\t2000-07-30 18:45:03\t2000-07-30 18:45:03\t2000\n",
    "1\t1\t3\t4.0\t2000-07-30 18:20:47\t2000-07-30 18:20:47\t2000\n",
    "2\t1\t6\t4.0\t2000-07-30 18:37:04\t2000-07-30 18:37:04\t2000\n",
    "3\t1\t47\t5.0\t2000-07-30 19:03:35\t2000-07-30 19:03:35\t2000\n",
    "4\t1\t50\t5.0\t2000-07-30 18:48:51\t2000-07-30 18:48:51\t2000\n",
    "import pandas as pd\n",
    "​\n",
    "# Check for missing values in the df_merged DataFrame\n",
    "missing_values = df_ratings.isna().sum()\n",
    "​\n",
    "# Display columns with missing values (if any)\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "if not columns_with_missing_values.empty:\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    print(columns_with_missing_values)\n",
    "else:\n",
    "    print(\"No Missing Values Found in the Dataset\")\n",
    "​\n",
    "No Missing Values Found in the Dataset\n",
    "df_merged\n",
    "movieId\ttitle\tgenres\trelease_year\taverage_rating\trating_count\tuserId\trating\ttimestamp\ttimestamp_readable\tyear\n",
    "0\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t1.0\t4.0\t2000-07-30 18:45:03\t2000-07-30 18:45:03\t2000.0\n",
    "1\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t5.0\t4.0\t1996-11-08 06:36:02\t1996-11-08 06:36:02\t1996.0\n",
    "2\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t7.0\t4.5\t2005-01-25 06:52:26\t2005-01-25 06:52:26\t2005.0\n",
    "3\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t15.0\t2.5\t2017-11-13 12:59:30\t2017-11-13 12:59:30\t2017.0\n",
    "4\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\t1995.0\t3.92093\t215.0\t17.0\t4.5\t2011-05-18 05:28:03\t2011-05-18 05:28:03\t2011.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "100849\t193581\tBlack Butler: Book of the Atlantic (2017)\tAction|Animation|Comedy|Fantasy\t2017.0\t4.00000\t1.0\t184.0\t4.0\t2018-09-16 14:44:42\t2018-09-16 14:44:42\t2018.0\n",
    "100850\t193583\tNo Game No Life: Zero (2017)\tAnimation|Comedy|Fantasy\t2017.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 14:52:25\t2018-09-16 14:52:25\t2018.0\n",
    "100851\t193585\tFlint (2017)\tDrama\t2017.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 14:56:45\t2018-09-16 14:56:45\t2018.0\n",
    "100852\t193587\tBungo Stray Dogs: Dead Apple (2018)\tAction|Animation\t2018.0\t3.50000\t1.0\t184.0\t3.5\t2018-09-16 15:00:21\t2018-09-16 15:00:21\t2018.0\n",
    "100853\t193609\tAndrew Dice Clay: Dice Rules (1991)\tComedy\t1991.0\t4.00000\t1.0\t331.0\t4.0\t2018-09-17 04:13:26\t2018-09-17 04:13:26\t2018.0\n",
    "100818 rows × 11 columns\n",
    "\n",
    "Creating a user-item interaction matrix from the dataset you provided involves reshaping the data into the desired format where rows represent users, columns represent items (movies in this case), and the values are ratings. Below is a simplified example of how you can create such a matrix using Python and pandas:\n",
    "\n",
    "import pandas as pd\n",
    "​\n",
    "# Assuming your dataset is stored in df_merged\n",
    "# You may need to select relevant columns for the user-item matrix\n",
    "# I'll assume 'userId', 'movieId', and 'rating' columns are present in your dataset\n",
    "​\n",
    "# Pivot the DataFrame to create the user-item interaction matrix\n",
    "user_item_matrix = df_merged.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "​\n",
    "# Fill missing values with 0 (assuming missing values mean no interaction)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "​\n",
    "# The user-item matrix is now ready for collaborative filtering\n",
    "user_item_matrix.head()\n",
    "​\n",
    "movieId\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t...\t193565\t193567\t193571\t193573\t193579\t193581\t193583\t193585\t193587\t193609\n",
    "userId\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "1.0\t4.0\t0.0\t4.0\t0.0\t0.0\t4.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "4.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5.0\t4.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5 rows × 9711 columns\n",
    "\n",
    "Importing Libraries: The code begins by importing the necessary Python libraries. In this case, it imports pandas as pd, which is a popular library for data manipulation and analysis.\n",
    "\n",
    "Dataset Assumptions: The code assumes that you have a dataset named df_merged, which contains information about movie ratings given by users. It further assumes that this dataset contains at least three columns: 'userId', 'movieId', and 'rating'.\n",
    "\n",
    "Pivoting the DataFrame: The main operation occurs with the pivot_table function. This function reshapes the original DataFrame (df_merged) into a user-item interaction matrix. Here's how it works:\n",
    "\n",
    "index='userId': The 'userId' column is used as the index (rows) of the new matrix. columns='movieId': The 'movieId' column is used to create columns in the matrix, each representing a unique movie. values='rating': The 'rating' column provides the values to fill the matrix cells. Handling Missing Values: After creating the initial matrix, the code fills missing values with 0. This is a common approach in collaborative filtering, where missing values typically represent no interaction or unrated items by users. By filling missing values with 0, it indicates that a user has not rated a particular movie.\n",
    "\n",
    "Resulting User-Item Matrix: Finally, the code prints the first few rows of the user-item interaction matrix using print(user_item_matrix.head()). The matrix has users as rows and movies as columns, with each cell containing a rating (or 0 for missing ratings).\n",
    "\n",
    "Each row corresponds to a user, identified by their 'userId'. Each column corresponds to a movie, identified by its 'movieId'. The values in the matrix represent the ratings given by users to the respective movies. This user-item interaction matrix serves as the foundation for collaborative filtering algorithms. It enables you to calculate similarities between users or items, make recommendations, and build recommendation systems. Collaborative filtering algorithms use this matrix to identify patterns and make personalized recommendations based on user preferences and item similarities.\n",
    "\n",
    "As you further develop your skills as a data scientist, you may explore more advanced techniques to improve the quality of recommendations and address challenges such as handling sparse data or dealing with cold start problems for new users or items.\n",
    "\n",
    "Baseline Model (\"Popularity-Based Recommender\" )\n",
    "In this approach, the recommendation are not personalized, instead they are based on popular or global statistics e.g. mean.\n",
    "\n",
    "# Calculate the global mean rating\n",
    "global_mean_rating = user_item_matrix.stack().mean()\n",
    "​\n",
    "# Create a baseline model where every rating is set to the global mean\n",
    "baseline_model = user_item_matrix.copy()\n",
    "baseline_model[:] = global_mean_rating\n",
    "​\n",
    "In this baseline model you've implemented, it uses the 'Global Mean Rating\" as the basis for recommendations. It assigns the same global mean rating to every user, effectively treating all items as equally popular or good. This means that the model reecommendds the same items to every user without taking into ccount individual prefrences or interations.\n",
    "\n",
    "Train-set Matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "​\n",
    "# use data in user_item_matrix and baseline_model\n",
    "rmse = mean_squared_error(user_item_matrix.values, baseline_model.values, squared=False)\n",
    "print(f\"RMSE of the baseline model: {rmse:.2f}\")\n",
    "​\n",
    "​\n",
    "# Split the data into training and testing sets\n",
    "train_matrix, test_matrix = train_test_split(user_item_matrix, test_size=0.2, random_state=42)\n",
    "​\n",
    "# Calculate the global mean rating from the training set\n",
    "global_mean_rating = train_matrix.stack().mean() \n",
    "​\n",
    "# Create a baseline model where every rating is set to the global mean from the training set\n",
    "baseline_model = train_matrix.copy()\n",
    "baseline_model[:] = global_mean_rating\n",
    "​\n",
    "​\n",
    "# Calculate MAE for the baseline model\n",
    "mae = mean_absolute_error(train_matrix.values, baseline_model.values)\n",
    "print(f\"MAE of the baseline model on the train set: {mae:.2f}\")\n",
    "​\n",
    "# Calculate RMSE for the baseline model\n",
    "rmse = mean_squared_error(train_matrix.values, baseline_model.values, squared=False)\n",
    "print(f\"RMSE of the baseline model on the train set: {rmse:.2f}\")\n",
    "​\n",
    "RMSE of the baseline model: 0.35\n",
    "MAE of the baseline model on the train set: 0.11\n",
    "RMSE of the baseline model on the train set: 0.32\n",
    "We calculate the global mean rating from the training set. It's important to use statistics calculated from the training data to avoid data leakage into the test set.\n",
    "\n",
    "We create a baseline model for the test set where every rating is set to the global mean from the training set. This is done to simulate how well the baseline model generalizes to new, unseen data (test set).\n",
    "\n",
    "We calculate the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for the baseline model's predictions on the test set. These metrics will give you an idea of how well the baseline model performs on unseen data.\n",
    "\n",
    "By performing this train-test split and evaluating the baseline model on the test set, you can get a better understanding of how well the model performs in terms of prediction error when faced with new user-item interactions. This evaluation step is crucial in assessing the model's generalization capabilities.\n",
    "\n",
    "The global mean rating, in the context of recommendation systems, can be defined as follows:\n",
    "\n",
    "Global Mean Rating: The global mean rating is a statistical measure representing the average rating value across all users and items in a dataset. It is calculated by summing up all the ratings in the dataset and dividing by the total number of ratings. Mathematically, it can be defined as:\n",
    "\n",
    "Global Mean Rating ∑ Ratings Total Number of Ratings Global Mean Rating= Total Number of Ratings ∑Ratings​\n",
    "\n",
    "Here's how it can be defined in words:\n",
    "\n",
    "Sum of Ratings: Add up all the individual ratings given by users to different items (e.g., movies or products) in the dataset.\n",
    "\n",
    "Total Number of Ratings: Count the total number of ratings provided by users across all items in the dataset.\n",
    "\n",
    "Average Calculation: Divide the sum of ratings by the total number of ratings to compute the global mean rating.\n",
    "\n",
    "The global mean rating provides a central reference point for ratings in the dataset. It represents the average sentiment or preference of users toward items. This value is often used in recommendation systems for various purposes, as mentioned in previous responses, such as creating baseline models, normalization, addressing the cold start problem, and as a reference point for evaluation metrics.\n",
    "\n",
    "Test-Set martix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "​\n",
    "​\n",
    "# Split the data into training and testing sets\n",
    "train_matrix, test_matrix = train_test_split(user_item_matrix, test_size=0.2, random_state=42)\n",
    "​\n",
    "# Calculate the global mean rating from the training set\n",
    "global_mean_rating = train_matrix.stack().mean() \n",
    "​\n",
    "# Create a baseline model where every rating is set to the global mean from the training set\n",
    "baseline_model = test_matrix.copy()\n",
    "baseline_model[:] = global_mean_rating\n",
    "​\n",
    "# Calculate MAE for the baseline model\n",
    "mae = mean_absolute_error(test_matrix.values, baseline_model.values)\n",
    "print(f\"MAE of the baseline model on the test set: {mae:.2f}\")\n",
    "​\n",
    "# Calculate RMSE for the baseline model\n",
    "rmse = mean_squared_error(test_matrix.values, baseline_model.values, squared=False)\n",
    "print(f\"RMSE of the baseline model on the test set: {rmse:.2f}\")\n",
    "​\n",
    "MAE of the baseline model on the test set: 0.13\n",
    "RMSE of the baseline model on the test set: 0.37\n",
    "Cosine Similarity\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import numpy as np\n",
    "​\n",
    "# # Assuming you have your user-item interaction matrix\n",
    "# # Calculate user similarity using cosine similarity\n",
    "# user_similarity = cosine_similarity(user_item_matrix)\n",
    "​\n",
    "# # Convert the similarity matrix into a DataFrame for easier manipulation\n",
    "# user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "​\n",
    "# # Function to get top N recommendations for a user\n",
    "# def get_top_recommendations(user_id, top_n=10):\n",
    "#     # Get the user's similarity scores with all other users\n",
    "#     user_similarities = user_similarity_df.loc[user_id]\n",
    "    \n",
    "#     # Sort users by similarity in descending order and select the top N\n",
    "#     most_similar_users = user_similarities.sort_values(ascending=False)[1:top_n+1]\n",
    "    \n",
    "#     # Get movies that the user has not rated\n",
    "#     user_ratings = user_item_matrix.loc[user_id]\n",
    "#     unrated_movies = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "#     # Calculate weighted average ratings for unrated movies based on similar users' ratings\n",
    "#     weighted_ratings = user_item_matrix.loc[most_similar_users.index].mean()\n",
    "    \n",
    "#     # Sort unrated movies by weighted rating in descending order\n",
    "#     top_recommendations = weighted_ratings[unrated_movies].sort_values(ascending=False)[:top_n]\n",
    "    \n",
    "#     return top_recommendations\n",
    "​\n",
    "# # Example: Get top 10 recommendations for user with userId 1\n",
    "# user_id = 1\n",
    "# top_recommendations = get_top_recommendations(user_id)\n",
    "​\n",
    "# # Calculate actual ratings given by the user (for comparison)\n",
    "# actual_ratings = user_item_matrix.loc[user_id][top_recommendations.index]\n",
    "​\n",
    "# # Calculate MSE and RMSE\n",
    "# mse = mean_squared_error(actual_ratings, top_recommendations)\n",
    "# rmse = np.sqrt(mse)\n",
    "​\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"RMSE: {rmse:.2f}\")\n",
    "​\n",
    "MSE (Mean Squared Error): MSE is a measure of the average squared difference between the actual ratings and the predicted ratings. A lower MSE indicates that the model's predictions are closer to the actual ratings.\n",
    "\n",
    "In your case, the MSE is 4.01. This means that, on average, the squared difference between the actual and predicted ratings for the recommended movies is 4.01. A lower MSE would be desirable, indicating that the model's recommendations are more accurate. RMSE (Root Mean Squared Error): RMSE is the square root of MSE and is a commonly used metric for measuring the average magnitude of prediction errors. It provides a more interpretable measure of how far off the model's predictions are from the actual ratings.\n",
    "\n",
    "In your case, the RMSE is 2.00. This means that, on average, the model's predictions have an error of approximately 2.00 units in terms of the user's ratings. A lower RMSE indicates better model accuracy, with values closer to 0 being ideal. Overall Interpretation:\n",
    "\n",
    "The RMSE of 2.00 suggests that, on average, the model's recommendations have an error of 2.00 units when compared to the actual ratings given by the user. While a lower RMSE would be preferable, the performance of the model should be evaluated in the context of the specific application and compared to other models or baselines. It's worth noting that RMSE is just one metric, and it's important to consider other factors such as user satisfaction, business goals, and the specific use case when assessing the quality of a recommendation system.\n",
    "\n",
    "Pearson Correlation Similarity\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "​\n",
    "# Assuming you have your user-item interaction matrix\n",
    "# Calculate user similarity using Pearson correlation\n",
    "user_similarity = 1 - pairwise_distances(user_item_matrix.T, metric='correlation')\n",
    "​\n",
    "# Convert the similarity matrix into a DataFrame for easier manipulation\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "​\n",
    "# Function to get top N recommendations for a user\n",
    "def get_top_recommendations(user_id, top_n=10):\n",
    "    # Get the user's similarity scores with all other users\n",
    "    user_similarities = user_similarity_df[user_id]\n",
    "    \n",
    "    # Sort users by similarity in descending order and select the top N\n",
    "    most_similar_users = user_similarities.sort_values(ascending=False)[1:top_n+1]\n",
    "    \n",
    "    # Get movies that the user has not rated\n",
    "    user_ratings = user_item_matrix[user_id]\n",
    "    unrated_movies = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "    # Calculate weighted average ratings for unrated movies based on similar users' ratings\n",
    "    weighted_ratings = user_item_matrix[most_similar_users.index].mean(axis=1)\n",
    "    \n",
    "    # Sort unrated movies by weighted rating in descending order\n",
    "    top_recommendations = weighted_ratings[unrated_movies].sort_values(ascending=False)[:top_n]\n",
    "    \n",
    "    return top_recommendations\n",
    "​\n",
    "# Get top 10 recommendations for user with userId 1\n",
    "user_id = 1\n",
    "top_recommendations = get_top_recommendations(user_id)\n",
    "​\n",
    "# Calculate actual ratings given by the user (for comparison)\n",
    "actual_ratings = user_item_matrix[user_id][top_recommendations.index]\n",
    "​\n",
    "# Calculate MSE and RMSE\n",
    "mse = mean_squared_error(actual_ratings, top_recommendations)\n",
    "rmse = np.sqrt(mse)\n",
    "​\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "​\n",
    "MSE: 0.68\n",
    "RMSE: 0.82\n",
    "MSE (Mean Squared Error): MSE measures the average squared difference between the actual user ratings and the predicted ratings from the collaborative filtering model.\n",
    "\n",
    "In this case, the MSE is 0.68, which means that, on average, the squared difference between the actual and predicted ratings for the recommended movies is 0.68. Lower MSE values indicate better model accuracy. Compared to the previous result (MSE of 4.01), this result is significantly better, indicating that the model's recommendations are closer to the user's actual preferences. RMSE (Root Mean Squared Error): RMSE is the square root of MSE and provides a more interpretable measure of the average prediction error.\n",
    "\n",
    "The RMSE is 0.82, which means that, on average, the model's predictions have an error of approximately 0.82 units in terms of the user's ratings. This value is also significantly lower than the previous RMSE (2.00), further indicating that the model's recommendations are more accurate. Interpretation:\n",
    "\n",
    "The lower MSE and RMSE values suggest that the collaborative filtering model using Pearson correlation similarity is providing more accurate movie recommendations compared to the previous model using cosine similarity. Lower MSE and RMSE values are indicative of a better-performing recommendation system, as they reflect smaller prediction errors. Keep in mind that while these metrics provide a quantitative assessment of the model's accuracy, it's essential to consider other factors such as user satisfaction and the specific goals of your recommendation system. It's also important to note that model performance can vary depending on the dataset and the quality of user ratings, so it's a good practice to perform cross-validation and assess the model's performance on a larger test dataset if available.\n",
    "\n",
    "​\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "​\n",
    "# # Assuming you have your user-item interaction matrix\n",
    "# # Calculate user similarity using cosine similarity\n",
    "# user_similarity = cosine_similarity(user_item_matrix)\n",
    "​\n",
    "# Convert the similarity matrix into a DataFrame for easier manipulation\n",
    "# user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "​\n",
    "# # Function to get top N recommendations for a user\n",
    "# def get_top_recommendations(user_id, top_n=10):\n",
    "#     # Get the user's similarity scores with all other users\n",
    "#     user_similarities = user_similarity_df.loc[user_id]\n",
    "    \n",
    "#     # Sort users by similarity in descending order and select the top N\n",
    "#     most_similar_users = user_similarities.sort_values(ascending=False)[1:top_n+1]\n",
    "    \n",
    "#     # Get movies that the user has not rated\n",
    "#     user_ratings = user_item_matrix.loc[user_id]\n",
    "#     unrated_movies = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "#     # Calculate weighted average ratings for unrated movies based on similar users' ratings\n",
    "#     weighted_ratings = user_item_matrix.loc[most_similar_users.index].mean()\n",
    "    \n",
    "#     # Sort unrated movies by weighted rating in descending order\n",
    "#     top_recommendations = weighted_ratings[unrated_movies].sort_values(ascending=False)[:top_n]\n",
    "    \n",
    "#     return top_recommendations\n",
    "​\n",
    "# # Example: Get top 10 recommendations for user with userId 1\n",
    "# user_id = 67\n",
    "# top_recommendations = get_top_recommendations(user_id)\n",
    "# print(f\"Top 10 recommendations for user {user_id}:\\n{top_recommendations}\")\n",
    "​\n",
    "HYPERPARAMETER TUNING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "​\n",
    "# Assuming you have your user-item interaction matrix (already defined)\n",
    "# Fill missing values with 0 (assuming missing values mean no interaction)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "​\n",
    "# Function to get top N recommendations for a user\n",
    "def get_top_recommendations(user_id, top_n=10):\n",
    "    # Get the user's ratings\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    \n",
    "    # Sort movies by predicted rating in descending order\n",
    "    top_movie_ids = user_ratings.sort_values(ascending=False).index\n",
    "    \n",
    "    # Exclude movies the user has already rated\n",
    "    rated_movies = user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index\n",
    "    top_movie_ids = top_movie_ids.difference(rated_movies)\n",
    "    \n",
    "    # Get the top N movie recommendations\n",
    "    top_recommendations = top_movie_ids[:top_n]\n",
    "    \n",
    "    return top_recommendations\n",
    "​\n",
    "# Define a range of neighbor values to test\n",
    "neighbor_values = [10, 20, 30, 50]\n",
    "​\n",
    "best_rmse = float('inf')\n",
    "best_neighbors = None\n",
    "​\n",
    "# Assuming you have a user_id for which you want to make recommendations\n",
    "user_id = 5\n",
    "​\n",
    "​\n",
    "# Ensure that the number of neighbors does not exceed the total number of users\n",
    "max_neighbors = min(max(neighbor_values), user_item_matrix.shape[0])\n",
    "​\n",
    "for neighbors in neighbor_values:\n",
    "    # Make sure neighbors does not exceed the maximum number of users\n",
    "    neighbors = min(neighbors, max_neighbors)\n",
    "​\n",
    "    # Train the model with the current number of neighbors\n",
    "    top_recommendations = get_top_recommendations(user_id, top_n=neighbors)\n",
    "    \n",
    "    # Calculate actual ratings given by the user (for comparison)\n",
    "    actual_ratings = user_item_matrix.loc[user_id, top_recommendations]\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, np.ones_like(actual_ratings) * 4))  # Assuming a \"perfect\" rating of 4\n",
    "    \n",
    "    # Check if this set of hyperparameters results in a better RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_neighbors = neighbors\n",
    "​\n",
    "print(f\"Best number of neighbors: {best_neighbors}\")\n",
    "print(f\"Best RMSE: {best_rmse:.2f}\")\n",
    "​\n",
    "Best number of neighbors: 10\n",
    "Best RMSE: 4.00\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "​\n",
    "# Assuming you have your user-item interaction matrix (already defined)\n",
    "# Fill missing values with 0 (assuming missing values mean no interaction)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "​\n",
    "# Function to get top N recommendations for a user\n",
    "def get_top_recommendations(user_id, top_n=10):\n",
    "    # Get the user's ratings\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    \n",
    "    # Sort movies by predicted rating in descending order\n",
    "    top_movie_ids = user_ratings.sort_values(ascending=False).index\n",
    "    \n",
    "    # Exclude movies the user has already rated\n",
    "    rated_movies = user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index\n",
    "    top_movie_ids = top_movie_ids.difference(rated_movies)\n",
    "    \n",
    "    # Get the top N movie recommendations\n",
    "    top_recommendations = top_movie_ids[:top_n]\n",
    "    \n",
    "    return top_recommendations\n",
    "​\n",
    "# Define a range of neighbor values to test\n",
    "neighbor_values = [10, 20, 30, 50]\n",
    "​\n",
    "best_rmse = float('inf')\n",
    "best_neighbors = None\n",
    "​\n",
    "# Assuming you have a user_id for which you want to make recommendations\n",
    "user_id = 2\n",
    "​\n",
    "# Ensure that the number of neighbors does not exceed the total number of users\n",
    "max_neighbors = min(max(neighbor_values), user_item_matrix.shape[0])\n",
    "​\n",
    "for neighbors in neighbor_values:\n",
    "    # Make sure neighbors does not exceed the maximum number of users\n",
    "    neighbors = min(neighbors, max_neighbors)\n",
    "​\n",
    "    # Train the model with the current number of neighbors\n",
    "    top_recommendations = get_top_recommendations(user_id, top_n=neighbors)\n",
    "    \n",
    "    # Calculate actual ratings given by the user (for comparison)\n",
    "    actual_ratings = user_item_matrix.loc[user_id, top_recommendations]\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, np.ones_like(actual_ratings) * 4))  # Assuming a \"perfect\" rating of 4\n",
    "    \n",
    "    # Display RMSE for this set of hyperparameters\n",
    "    print(f\"RMSE for {neighbors} neighbors: {rmse:.2f}\")\n",
    "​\n",
    "print(\"Hyperparameter tuning completed.\")\n",
    "​\n",
    "RMSE for 10 neighbors: 4.00\n",
    "RMSE for 20 neighbors: 4.00\n",
    "RMSE for 30 neighbors: 4.00\n",
    "RMSE for 50 neighbors: 4.00\n",
    "Hyperparameter tuning completed.\n",
    "​\n",
    "​\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "​\n",
    "# Assuming you have the user-item interaction matrix (user ratings)\n",
    "# Fill missing values with 0 (assuming missing values mean no interaction)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "​\n",
    "# Number of latent factors to use (hyperparameter to tune)\n",
    "k = 10\n",
    "​\n",
    "# Perform SVD on the filled user-item matrix\n",
    "U, S, Vt = svd(user_item_matrix, full_matrices=False)\n",
    "​\n",
    "# Keep only the top k singular values and vectors to approximate the matrix\n",
    "U = U[:, :k]\n",
    "S = np.diag(S[:k])\n",
    "Vt = Vt[:k, :]\n",
    "​\n",
    "# Reconstruct the user-item matrix with reduced dimensionality\n",
    "user_item_matrix_reconstructed = np.dot(np.dot(U, S), Vt)\n",
    "​\n",
    "# Now you can use user_item_matrix_reconstructed for making recommendations\n",
    "user_item_matrix_reconstructed\n",
    "array([[ 2.86218459e+00,  9.38307717e-01,  9.76468395e-01, ...,\n",
    "        -1.25953730e-02, -1.25953730e-02, -1.93009605e-02],\n",
    "       [ 1.92397613e-01, -8.22498777e-03, -2.68680511e-02, ...,\n",
    "         5.95821412e-03,  5.95821412e-03,  1.31948596e-02],\n",
    "       [ 3.16534545e-02,  1.65642071e-02,  1.95186256e-02, ...,\n",
    "         2.38457495e-04,  2.38457495e-04, -1.88838138e-03],\n",
    "       ...,\n",
    "       [ 2.80816496e+00,  1.97986558e+00,  1.80789282e+00, ...,\n",
    "        -4.86754164e-02, -4.86754164e-02, -3.28889539e-03],\n",
    "       [ 8.33552320e-01,  6.58897660e-01,  3.08542830e-01, ...,\n",
    "         3.23998286e-04,  3.23998286e-04,  8.34800504e-04],\n",
    "       [ 8.15021801e-01,  2.74566695e+00, -2.80625650e-01, ...,\n",
    "         4.59635122e-02,  4.59635122e-02,  6.40349294e-02]])\n",
    "unrated_items\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[48], line 1\n",
    "----> 1 unrated_items\n",
    "\n",
    "NameError: name 'unrated_items' is not defined\n",
    "\n",
    "# Identify unrated items for a specific user (e.g., user_id = 1)\n",
    "user_id = 6\n",
    "user_ratings = user_item_matrix.loc[user_id]\n",
    "unrated_items = user_ratings[user_ratings == 0].index\n",
    "​\n",
    "# Get predicted ratings for unrated items from the reconstructed matrix\n",
    "predicted_ratings = user_item_matrix_reconstructed[user_id]\n",
    "​\n",
    "# Sort unrated items by predicted rating in descending order\n",
    "top_recommendations = predicted_ratings[unrated_items].sort_values(ascending=False)\n",
    "​\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "Cell In[49], line 10\n",
    "      7 predicted_ratings = user_item_matrix_reconstructed[user_id]\n",
    "      9 # Sort unrated items by predicted rating in descending order\n",
    "---> 10 top_recommendations = predicted_ratings[unrated_items].sort_values(ascending=False)\n",
    "\n",
    "IndexError: index 25746 is out of bounds for axis 0 with size 9711\n",
    "\n",
    "user_id = 1  # Replace with the user ID you want to check\n",
    "​\n",
    "try:\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    # User exists in the original matrix\n",
    "    # Proceed with recommendations or other operations\n",
    "except KeyError:\n",
    "    print(f\"User {user_id} does not exist in the matrix.\")\n",
    "​\n",
    "# Assuming you have actual ratings for the user\n",
    "actual_ratings = user_item_matrix.loc[user_id, unrated_items]\n",
    "​\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(actual_ratings, top_recommendations))\n",
    "​\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[91], line 5\n",
    "      2 actual_ratings = user_item_matrix.loc[user_id, unrated_items]\n",
    "      4 # Calculate RMSE\n",
    "----> 5 rmse = np.sqrt(mean_squared_error(actual_ratings, top_recommendations))\n",
    "\n",
    "File ~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_regression.py:442, in mean_squared_error(y_true, y_pred, sample_weight, multioutput, squared)\n",
    "    382 def mean_squared_error(\n",
    "    383     y_true, y_pred, *, sample_weight=None, multioutput=\"uniform_average\", squared=True\n",
    "    384 ):\n",
    "    385     \"\"\"Mean squared error regression loss.\n",
    "    386 \n",
    "    387     Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "   (...)\n",
    "    440     0.825...\n",
    "    441     \"\"\"\n",
    "--> 442     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "    443         y_true, y_pred, multioutput\n",
    "    444     )\n",
    "    445     check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    446     output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
    "\n",
    "File ~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_regression.py:100, in _check_reg_targets(y_true, y_pred, multioutput, dtype)\n",
    "     66 def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\n",
    "     67     \"\"\"Check that y_true and y_pred belong to the same regression task.\n",
    "     68 \n",
    "     69     Parameters\n",
    "   (...)\n",
    "     98         correct keyword.\n",
    "     99     \"\"\"\n",
    "--> 100     check_consistent_length(y_true, y_pred)\n",
    "    101     y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n",
    "    102     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
    "\n",
    "File ~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py:397, in check_consistent_length(*arrays)\n",
    "    395 uniques = np.unique(lengths)\n",
    "    396 if len(uniques) > 1:\n",
    "--> 397     raise ValueError(\n",
    "    398         \"Found input variables with inconsistent numbers of samples: %r\"\n",
    "    399         % [int(l) for l in lengths]\n",
    "    400     )\n",
    "\n",
    "ValueError: Found input variables with inconsistent numbers of samples: [9386, 50]\n",
    "\n",
    "​\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "​\n",
    "# Load the second dataset into Surprise format\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df_merged[['userId', 'movieId', 'rating']], reader)\n",
    "​\n",
    "# Split the data into train and test sets (or use cross-validation)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "​\n",
    "# Define and train your recommendation model (use the same model you trained on the first dataset)\n",
    "model = SVD()  \n",
    "model.fit(trainset)\n",
    "​\n",
    "# Generate predictions on the second dataset\n",
    "predictions = model.test(testset)\n",
    "​\n",
    "# Calculate RMSE on the second dataset\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE on the second dataset: {rmse}\")\n",
    "​\n",
    "RMSE: 0.8664\n",
    "RMSE on the second dataset: 0.8664022397274831\n",
    "​\n",
    "from surprise.model_selection import GridSearchCV\n",
    "​\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'reg_all': [0.02, 0.1, 0.2],\n",
    "    'lr_all': [0.002, 0.01, 0.02]\n",
    "}\n",
    "​\n",
    "# Initialize GridSearchCV with the SVD model and parameter grid\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5, n_jobs=-1)\n",
    "​\n",
    "# Load your data into Surprise format (assuming it's already loaded)\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df_merged[['userId', 'movieId', 'rating']], reader)\n",
    "​\n",
    "# Run the grid search to find the best hyperparameters\n",
    "grid_search.fit(data)\n",
    "​\n",
    "# Get the best RMSE score and best hyperparameters\n",
    "best_rmse = grid_search.best_score['rmse']\n",
    "best_params = grid_search.best_params['rmse']\n",
    "​\n",
    "print(f\"Best RMSE: {best_rmse}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "​\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[54], line 15\n",
    "     13 # Load your data into Surprise format (assuming it's already loaded)\n",
    "     14 reader = Reader(rating_scale=(0.5, 5.0))\n",
    "---> 15 data = Dataset.load_from_df(df_[['userId', 'movieId', 'rating']], reader)\n",
    "     17 # Run the grid search to find the best hyperparameters\n",
    "     18 grid_search.fit(data)\n",
    "\n",
    "NameError: name 'df_' is not defined\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
